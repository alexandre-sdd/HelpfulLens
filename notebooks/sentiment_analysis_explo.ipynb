{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# new_file.py\n",
    "# GitHub Copilot\n",
    "# Basic EDA + baseline sentiment using VADER on data/cleaned/reviews_clean.parquet\n",
    "# Saves augmented dataframe to data/derived/reviews_sentiment.parquet\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try to import repo helpers in src if present (optional)\n",
    "try:\n",
    "    import src  # noqa: F401\n",
    "except Exception:\n",
    "    src = None\n",
    "\n",
    "# Load data\n",
    "data_dir = Path(\"/Users/alexandresepulvedadedietrich/Code/HelpfulLens/data\")\n",
    "input_path = Path(data_dir / \"datasets/training/yelp_helpfulness_train.parquet\")\n",
    "df_clean = pd.read_parquet(input_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_clean.copy().sample(n=100000, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify text column heuristically\n",
    "text_cols = [c for c in df.columns if any(k in c.lower() for k in (\"review\", \"text\", \"comment\"))]\n",
    "if not text_cols:\n",
    "    raise ValueError(\"No text-like column found in dataframe. Columns: \" + \", \".join(df.columns))\n",
    "text_col = text_cols[0]\n",
    "\n",
    "# Basic EDA\n",
    "print(\"Loaded:\", input_path, \"shape:\", df.shape)\n",
    "print(\"Text column used:\", text_col)\n",
    "print(\"\\nSample rows:\")\n",
    "print(df[[text_col]].head(5))\n",
    "\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum())\n",
    "\n",
    "# Text features\n",
    "df[\"_text\"] = df[text_col].astype(str)\n",
    "df[\"_char_len\"] = df[\"_text\"].str.len()\n",
    "df[\"_word_count\"] = df[\"_text\"].str.split().map(lambda x: len(x) if isinstance(x, list) else 0)\n",
    "df[\"_avg_word_len\"] = df[\"_text\"].apply(lambda s: np.mean([len(w) for w in re.findall(r\"\\w+\", s)]) if s else 0)\n",
    "\n",
    "print(\"\\nText length stats:\")\n",
    "print(df[[\"_char_len\", \"_word_count\", \"_avg_word_len\"]].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Baseline sentiment: VADER\n",
    "try:\n",
    "    try:\n",
    "        nltk.data.find(\"sentiment/vader_lexicon.zip\")\n",
    "    except LookupError:\n",
    "        nltk.download(\"vader_lexicon\", quiet=True)\n",
    "except Exception:\n",
    "    # fallback to vaderSentiment package\n",
    "    print(\"NLTK vader_lexicon not found, using vaderSentiment package.\")\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_scores(text):\n",
    "    return sia.polarity_scores(str(text))\n",
    "\n",
    "scores = df[\"text\"].map(vader_scores)\n",
    "scores_df = pd.DataFrame(list(scores))\n",
    "df = pd.concat([df.reset_index(drop=True), scores_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "# Label mapping (standard VADER thresholds)\n",
    "def vader_label(compound):\n",
    "    if compound >= 0.05:\n",
    "        return \"positive\"\n",
    "    if compound <= -0.05:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "df[\"_vader_label\"] = df[\"compound\"].apply(vader_label)\n",
    "\n",
    "print(\"\\nVADER label distribution:\")\n",
    "print(df[\"_vader_label\"].value_counts(normalize=False))\n",
    "\n",
    "# Quick visualization (will work in notebook)\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.countplot(data=df, x=\"_vader_label\", order=[\"positive\",\"neutral\",\"negative\"], stat=\"percent\")\n",
    "plt.title(\"Baseline VADER sentiment distribution\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Top tokens (simple frequency)\n",
    "def tokenize(text):\n",
    "    return re.findall(r\"\\w{2,}\", text.lower())\n",
    "\n",
    "all_tokens = Counter()\n",
    "df[\"text\"].dropna().map(tokenize).map(all_tokens.update)\n",
    "top_tokens = all_tokens.most_common(30)\n",
    "print(\"\\nTop tokens:\", top_tokens[:20])\n",
    "\n",
    "# Save augmented dataset\n",
    "out_dir = Path(\"../data/derived\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "out_path = out_dir / \"reviews_sentiment.parquet\"\n",
    "df.to_parquet(out_path, index=False)\n",
    "print(\"\\nSaved augmented dataframe to:\", out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation between 'useful' and VADER sentiment metrics\n",
    "vader_metrics = [\"neg\", \"neu\", \"pos\", \"compound\"]\n",
    "correlations = df[[\"useful_rate_smoothed\"] + vader_metrics].corr().loc[\"useful_rate_smoothed\", vader_metrics]\n",
    "print(\"Correlation between 'useful_rate_smoothed' and VADER metrics:\")\n",
    "print(correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df[df[\"_vader_label\"] == \"negative\"][\"text\"]:\n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
