{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Yelp Review Usefulness Exploration\n",
    "\n",
    "Notebook focused on understanding and normalizing the Yelp `useful` vote signal so it can power downstream NLP feature engineering and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "Load core libraries for data manipulation, visualization, and path management."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Data Paths\n",
    "Point to the cleaned parquet datasets generated by the preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path(\"/Users/alexandresepulvedadedietrich/Code/HelpfulLens/data\")\n",
    "data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Load Reviews and Business Metadata\n",
    "The reviews table carries text, stars, usefulness, and dates, while the business table supplies categories and review counts that serve as exposure proxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = pd.read_parquet(data_dir / \"cleaned\" / \"reviews_clean.parquet\")\n",
    "df_business = pd.read_parquet(data_dir / \"cleaned\" / \"business_clean.parquet\")\n",
    "print(f\"Reviews: {df_reviews.shape}\")\n",
    "print(f\"Businesses: {df_business.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Clean the Target Signal\n",
    "Treat `useful == -1` as missing so raw vote counts remain untouched for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.copy()\n",
    "df_reviews.loc[df_reviews['useful'] == -1, 'useful'] = np.nan\n",
    "df_reviews[['useful']].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Explore Raw Usefulness\n",
    "Inspect the spread of usefulness values via quantile bins and visualize the distribution against star ratings without transforming the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['useful_quantile'] = pd.qcut(df_reviews['useful'], q=10, duplicates='drop')\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "df_reviews['useful_quantile'].value_counts().sort_index().plot(kind='bar')\n",
    "plt.title('Usefulness Distribution Across Quantiles')\n",
    "plt.ylabel('Count')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "subset = df_reviews.copy()\n",
    "subset['useful_plot'] = subset['useful'].fillna(0)\n",
    "upper = subset['useful_plot']\n",
    "sns.violinplot(x=subset['stars'], y=subset['useful_plot'], cut=0)\n",
    "plt.ylim(0, upper)\n",
    "plt.title('Distribution of useful votes by star rating (trimmed 95th percentile)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns  # pip install seaborn if needed\n",
    "\n",
    "# 1. Keep only valid rows (>= 0 on useful as your filter)\n",
    "cols = [\"useful\", \"funny\", \"cool\"]\n",
    "df_votes = df_reviews[df_reviews[\"useful\"] >= 0].copy()\n",
    "\n",
    "# 2. Clip extreme tails at 99th percentile so the violins are readable\n",
    "upper = df_votes[cols].quantile(0.99)\n",
    "for c in cols:\n",
    "    df_votes[c] = df_votes[c].clip(upper=upper[c])\n",
    "\n",
    "# 3. Log-transform: log(1 + votes)\n",
    "for c in cols:\n",
    "    df_votes[f\"log1p_{c}\"] = np.log1p(df_votes[c])\n",
    "\n",
    "# 4. Sample for plotting (7M rows is too much to render)\n",
    "df_sample = df_votes.sample(100_000, random_state=0)\n",
    "\n",
    "# 5. Long format for seaborn\n",
    "df_long = df_sample.melt(\n",
    "    value_vars=[f\"log1p_{c}\" for c in cols],\n",
    "    var_name=\"vote_type\",\n",
    "    value_name=\"log1p_votes\",\n",
    ")\n",
    "\n",
    "df_long[\"vote_type\"] = df_long[\"vote_type\"].str.replace(\"log1p_\", \"\")\n",
    "\n",
    "# 6. Violin plot\n",
    "plt.figure(figsize=(7, 4))\n",
    "sns.violinplot(\n",
    "    data=df_long,\n",
    "    x=\"vote_type\",\n",
    "    y=\"log1p_votes\",\n",
    "    cut=0,\n",
    "    scale=\"width\",\n",
    "    inner=\"quartile\",\n",
    "    palette=\"Set2\",\n",
    ")\n",
    "plt.xlabel(\"Vote type\")\n",
    "plt.ylabel(\"log(1 + votes)\")\n",
    "plt.title(\"Distribution of votes per review (log-transformed)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Merge Exposure Proxies\n",
    "Join review rows with their business metadata, compute review age, and retain signals that approximate exposure (business popularity plus time on site)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_reviews.merge(\n",
    "    df_business[['business_id', 'categories', 'review_count']],\n",
    "    on='business_id',\n",
    "    how='left',\n",
    ")\n",
    "\n",
    "df_reviews['date'] = pd.to_datetime(df_reviews['date'])\n",
    "max_date = df_reviews['date'].max()\n",
    "df_reviews['age_days'] = (max_date - df_reviews['date']).dt.days\n",
    "\n",
    "df_reviews[['review_count', 'age_days']].describe(percentiles=[0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Relative Usefulness Within Each Business\n",
    "Normalize `useful` by ranking reviews against others from the same business and by scaling against business popularity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews['useful_rank_in_business'] = (\n",
    "    df_reviews.groupby('business_id')['useful']\n",
    "    .rank(method='max', ascending=True)\n",
    ")\n",
    "\n",
    "df_reviews['useful_percentile_in_business'] = (\n",
    "    df_reviews.groupby('business_id')['useful']\n",
    "    .rank(method='average', pct=True)\n",
    ")\n",
    "\n",
    "df_reviews['useful_per_100_reviews'] = (\n",
    "    df_reviews['useful'] / (df_reviews['review_count'] + 1) * 100\n",
    ")\n",
    "\n",
    "df_reviews[['business_id', 'stars', 'useful', 'useful_percentile_in_business', 'useful_per_100_reviews']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Inspect a Random Business\n",
    "Sample one business with valid usefulness data, anonymize the identifier for display, and inspect reviews sorted by their within-business rank."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_businesses = df_reviews[df_reviews['useful'].notna()]['business_id'].unique()\n",
    "if len(valid_businesses) == 0:\n",
    "    raise ValueError('No businesses with valid usefulness data were found.')\n",
    "\n",
    "biz_id = pd.Series(valid_businesses).sample(1, random_state=42).iloc[0]\n",
    "subset = (\n",
    "    df_reviews.loc[df_reviews['business_id'] == biz_id]\n",
    "    .sort_values('useful_rank_in_business', ascending=True)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "business_label = f\"business_{np.random.randint(1_000_000)}\"\n",
    "subset.index = pd.MultiIndex.from_arrays(\n",
    "    [[business_label] * len(subset), subset['useful_rank_in_business']],\n",
    "    names=['business_display_id', 'useful_rank_in_business']\n",
    ")\n",
    "\n",
    "subset[['review_id', 'stars', 'useful', 'useful_percentile_in_business','useful_rank_in_business' , 'text']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Roadmap for Exposure-Aware Modeling\n",
    "\n",
    "1. **Clean and log-transform the target**: keep treating `useful == -1` as missing and rely on `log1p` for numerical stability.\n",
    "2. **Engineer exposure proxies**: review age and business review count serve as immediate signals; add traffic estimates if they become available.\n",
    "3. **Build exposure-aware metrics**: values such as `useful_rank_in_business`, `useful_per_100_reviews`, and Bayesian shrinkage estimates (`bayes_q`) provide normalized targets.\n",
    "4. **Filter by review age**: compare reviews only after a minimum age (e.g., 90 days) to reduce exposure imbalance.\n",
    "5. **Model usefulness with context features**: start with logistic regression on `stars`, `age_days`, and `review_count`; evaluate via ROC-AUC.\n",
    "6. **Derive intrinsic usefulness scores**: score every review at reference exposure settings to estimate text-driven usefulness independent of visibility.\n",
    "7. **Expand feature space**: add simple text statistics (length, sentence counts) before moving to embeddings or TFâ€“IDF matrices.\n",
    "8. **Incorporate Bayesian shrinkage**: model `useful` as Poisson with rate `q_i * exposure_i` to obtain regularized usefulness-per-exposure scores.\n",
    "\n",
    "This roadmap keeps the notebook focused on understanding the usefulness signal while leaving room for richer NLP-driven modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews[\"impressions\"] = df_reviews[\"funny\"] + df_reviews[\"cool\"] + df_reviews[\"useful\"]\n",
    "\n",
    "df_reviews[\"ratio_useful_impressions\"] = df_reviews[\"useful\"] / (df_reviews[\"impressions\"] + 1)\n",
    "df_reviews[\"ratio_funny_impressions\"] = df_reviews[\"funny\"] / (df_reviews[\"impressions\"] + 1)\n",
    "df_reviews[\"ratio_cool_impressions\"] = df_reviews[\"cool\"] / (df_reviews[\"impressions\"] + 1)\n",
    "\n",
    "vote_corr = df_reviews[[\"useful\", \"funny\", \"cool\", \"impressions\",\n",
    "                        \"ratio_useful_impressions\", \"ratio_funny_impressions\",\n",
    "                        \"ratio_cool_impressions\"]].corr(method=\"spearman\")\n",
    "print(\"\\nSpearman correlation between votes and impressions:\")\n",
    "print(vote_corr[\"useful\"].sort_values(ascending=False))\n",
    "\n",
    "sns.heatmap(vote_corr, annot=True, fmt=\".2f\", cmap=\"coolwarm\")\n",
    "plt.title(\"Spearman Correlation Heatmap of Votes and Impressions\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews.useful.fillna(-1, inplace=True)\n",
    "df_reviews[['useful', 'impressions', 'ratio_useful_impressions',\n",
    "            'ratio_funny_impressions', 'ratio_cool_impressions']].describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
